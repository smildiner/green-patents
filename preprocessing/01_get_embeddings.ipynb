{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2839077a",
   "metadata": {},
   "source": [
    "# Patent Breakthrough walkthrough\n",
    "\n",
    "This notebook illustrates the complete analysis process of breakthrough patents, from preparing input files\n",
    "to calculating impact and novelty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317e63",
   "metadata": {},
   "source": [
    "## 1. Preparing input files\n",
    "\n",
    "There are three input files: a file with patent texts, a patent/year-index, and list of patent/CPC-codes.\n",
    "\n",
    "### Patents\n",
    "In its raw format, the input file contains the text of one patent file per line.\n",
    "Each line starts with a path pointing to that patent's original text \n",
    "file (`/Volumes/External/txt/0000000-0100000/US1009.txt`), followed by the patent text. Example file: `./data/raw_input.txt`. \n",
    "\n",
    "\n",
    "### Patent/Year-index\n",
    "Contains the year of publication of each patent. Example file: `./data/year.csv`. \n",
    "\n",
    "\n",
    "### CPC-file\n",
    "The CPC-file (Cooperative Patent Classification) contains the patent classification code for each patent. These codes are used to calculate benchmark similarities. Example file: `./data/GPCPCs.txt`\n",
    "\n",
    "Note: the included data files only contain a small subset of the original data, for example purposes.\n",
    "\n",
    "#### Other files\n",
    "The three other files in the data folder - `greek.txt`  `stopwords.txt`, and `symbols.txt` - are required by the `OldPreprocessor`-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1048ccd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{home_path}/output\")\n",
    "output_fp = Path(f\"{home_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{home_path}/results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69345ab9",
   "metadata": {},
   "source": [
    "### 1.1. Compressing\n",
    "\n",
    "The compressor function transforms the patents to a more manageable format, sorts and saves them by year of publication, and compresses the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0ffaf2-33b6-4ab3-af74-2502e7fd4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from typing import List, Union, Dict\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import multiprocessing\n",
    "\n",
    "def read_xz(compressed_fp: Union[Path, str]) -> List[Dict]:\n",
    "    \"\"\"Read an .xz file containing patents\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    compressed_fp:\n",
    "        File to read the patents from.\n",
    "\n",
    "    Results\n",
    "    -------\n",
    "    patents: List[Dict]\n",
    "        Patents in the file.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"rb\") as handle:\n",
    "        patents = json.loads(handle.read().decode(encoding=\"utf-8\"))\n",
    "    return patents\n",
    "\n",
    "def write_xz(compressed_fp: Union[Path, str], patents: List[Dict]) -> None:\n",
    "    \"\"\"Write a set of patents to a compressed file\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    fp:\n",
    "        File to write to.\n",
    "    patents:\n",
    "        Patents to store.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"wb\", preset=9) as handle:\n",
    "        handle.write(str.encode(json.dumps(patents), encoding=\"utf-8\"))\n",
    "\n",
    "def parse_patent_file(patent_input_fp: str, year_lookup: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a raw patent file into a structured list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    patent_input_fp : str\n",
    "        Path to the .tsv input file to process with columns 'patent_id' and 'summary_text', one patent per row.\n",
    "\n",
    "    year_lookup : str\n",
    "        Path to the .csv file to lookup the year for each patent ID with columns 'pat' (patient_id) and 'year'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict[str, str]]\n",
    "        A sorted list of dictionaries, where each item is for one patent and\n",
    "        the year of publication:\n",
    "        {\n",
    "            'patent': patent_id,\n",
    "            'file': file name,\n",
    "            'contents':  summary_text,\n",
    "            'year': year\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting to parse file {patent_input_fp}.\\n\")\n",
    "    \n",
    "    # Increase the field size limit\n",
    "    csv.field_size_limit(10**8)\n",
    "    \n",
    "    # Initialize an empty list to store the structured patent data\n",
    "    patents = []\n",
    "\n",
    "    # Read the year lookup data into a dictionary for quick access\n",
    "    year_dict = {}\n",
    "    with open(year_lookup, 'r', newline='') as year_file:\n",
    "        year_reader = csv.DictReader(year_file, delimiter='\\t')\n",
    "        for row in year_reader:\n",
    "            year_dict[row['pat']] = row['year']  # Convert 'year' to integer\n",
    "\n",
    "    # Extract year from the filename\n",
    "    year = str(patent_input_fp).split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Read the raw patent file and parse the data\n",
    "    with open(patent_input_fp, 'r', newline='') as patent_file:\n",
    "        patent_reader = csv.DictReader(patent_file, delimiter='\\t')\n",
    "        for row in tqdm(patent_reader):\n",
    "            patent_id = row['patent_id']\n",
    "            \n",
    "            # Skip entries where \"patent_id\" is not an integer\n",
    "            if not patent_id.isdigit():\n",
    "                continue\n",
    "\n",
    "            # Skip entries where \"patent_id\" is not in year_file:\n",
    "            if patent_id not in year_dict:\n",
    "                continue\n",
    "\n",
    "            summary_text = row['summary_text']\n",
    "            # year = year_dict.get(patent_id, None)\n",
    "\n",
    "            # Create a dictionary for the current patent\n",
    "            patent_data = {\n",
    "                'patent': int(patent_id),  # Convert 'patent_id' to integer\n",
    "                'file': str(patent_input_fp),\n",
    "                'contents': summary_text,\n",
    "                'year': int(year)\n",
    "            }\n",
    "\n",
    "            # Append the patent data to the list\n",
    "            patents.append(patent_data)\n",
    "    \n",
    "    patents = sorted(patents, key=lambda x: x[\"patent\"])\n",
    "    \n",
    "    return patents\n",
    "\n",
    "def compress_raw_dir(patent_input_dir: Union[Path, str], year_fp: Union[Path, str], output_dir: Union[Path, str]) -> None:\n",
    "    \"\"\"Compress all raw files in a directory.\n",
    "\n",
    "    For efficiency, it stores which files have already been processed in\n",
    "    a file called 'processed_files.txt' in the output directory.\n",
    "    If somehow there is corruption, or re-runs are required, simply\n",
    "    delete this file.\n",
    "\n",
    "    This function is not thread-safe.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    patent_input_dir:\n",
    "        Directory containing all raw files with patents.\n",
    "    year_fp:\n",
    "        CSV file with publication year for each patent.\n",
    "    output_dir:\n",
    "        Directory to write the compressed files to.\n",
    "    \"\"\"\n",
    "    patent_input_dir = Path(patent_input_dir)\n",
    "    \n",
    "    for file in patent_input_dir.glob(\"g_brf_sum_text_*.tsv\"):\n",
    "        \n",
    "        # Extract year from the filename\n",
    "        year = str(file).split('_')[-1].split('.')[0]\n",
    "        \n",
    "        # Get next file path to process and compress\n",
    "        patent_input_fp = file\n",
    "        compressed_fp = output_dir / Path(str(year) + \".xz\")\n",
    "        \n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(compressed_fp):\n",
    "            print(f\"Output file {compressed_fp} already exists. Skipping to the next input file.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Parse patents for a year in the right format\n",
    "        patents = parse_patent_file(patent_input_fp, year_fp)\n",
    "\n",
    "        # Write the patents to a files, numbered by year\n",
    "        write_xz(compressed_fp, patents)\n",
    "\n",
    "\n",
    "def compress_and_write(patent_input_fp: Path, year_fp: Path, output_dir: Path) -> None:\n",
    "    \"\"\"Compresses patents from a single file and writes the compressed data to a new file.\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    patent_input_fp (Path):\n",
    "        Path to the input patent file.\n",
    "    year_fp (Path):\n",
    "        Path to the CSV file with publication year for each patent.\n",
    "    output_dir (Path):\n",
    "        Directory to write the compressed files to.\n",
    "    \"\"\"\n",
    "    year = str(patent_input_fp).split('_')[-1].split('.')[0]\n",
    "    compressed_fp = output_dir / Path(str(year) + \".xz\")\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(compressed_fp):\n",
    "        print(f\"Output file {compressed_fp} already exists. Skipping to the next input file.\\n\")\n",
    "        return\n",
    "\n",
    "    # Parse patents for a year in the right format\n",
    "    patents = parse_patent_file(patent_input_fp, year_fp)\n",
    "\n",
    "    # Write the patents to a file, compressed\n",
    "    write_xz(compressed_fp, patents)\n",
    "\n",
    "\n",
    "def compress_raw_dir_parallel(patent_input_dir: Union[Path, str], year_fp: Union[Path, str], output_dir: Union[Path, str], num_cores: int) -> None:\n",
    "    \"\"\"Compresses patents from multiple files in parallel using multiprocessing.\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    patent_input_dir (Union[Path, str]):\n",
    "        Directory containing all raw files with patents.\n",
    "    year_fp (Union[Path, str]):\n",
    "        CSV file with publication year for each patent.\n",
    "    output_dir (Union[Path, str]):\n",
    "        Directory to write the compressed files to.\n",
    "    num_cores (int):\n",
    "        Number of CPU cores to use for parallel processing.\n",
    "    \"\"\"\n",
    "    patent_input_dir = Path(patent_input_dir)\n",
    "    year_fp = Path(year_fp)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    patent_files = list(patent_input_dir.glob(\"g_brf_sum_text_*.tsv\"))\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "        pool.starmap(compress_and_write, [(file, year_fp, output_dir) for file in patent_files])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d823caaa-6a5e-49f1-abac-18d3cd78a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1976.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1977.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1978.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1979.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1982.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1984.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1983.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1989.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1987.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1985.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1990.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1992.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1981.tsv.\n",
      "\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1980.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1988.tsv.\n",
      "\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1986.tsv.\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1993.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1994.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1991.tsv.\n",
      "\n",
      "\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1995.tsv.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47720it [00:05, 10114.50it/s]\n",
      "57391it [00:06, 9552.44it/s]]\n",
      "58279it [00:06, 9712.30it/s] \n",
      "62104it [00:06, 9659.84it/s] \n",
      "66172it [00:06, 9959.97it/s] \n",
      "65427it [00:06, 10114.30it/s]\n",
      "66234it [00:06, 9535.57it/s]\n",
      "67612it [00:07, 9624.84it/s]\n",
      "70142it [00:06, 10112.53it/s]\n",
      "71419it [00:07, 9435.19it/s]\n",
      "72098it [00:07, 9398.26it/s]\n",
      "83406it [00:08, 9403.64it/s]\n",
      "78498it [00:08, 8864.76it/s]\n",
      "90553it [00:10, 8874.17it/s]\n",
      "95986it [00:10, 8839.71it/s]\n",
      "96604it [00:10, 8842.05it/s]\n",
      "97680it [00:11, 8677.97it/s]\n",
      "102590it [00:12, 8216.27it/s]\n",
      "99258it [00:11, 8375.80it/s]\n",
      "102227it [00:13, 7863.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1996.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110401it [00:13, 8014.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1997.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1999.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112703it [00:13, 8318.17it/s]\n",
      "35560it [00:03, 8776.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_1998.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74055it [00:08, 8439.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2000.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154508it [00:18, 8534.18it/s]\n",
      "145416it [00:17, 7825.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2001.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148494it [00:18, 8214.93it/s]\n",
      "158772it [00:20, 7906.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2002.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58949it [00:07, 7747.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2003.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59755it [00:06, 8468.14it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2004.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167290it [00:21, 7822.78it/s]\n",
      "169033it [00:20, 8081.84it/s]\n",
      "170519it [00:21, 8044.16it/s]\n",
      "165712it [00:19, 8511.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2005.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2006.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144865it [00:18, 7911.96it/s]\n",
      "175505it [00:22, 7693.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2007.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158897it [00:20, 7872.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2008.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159675it [00:20, 7926.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2009.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168915it [00:21, 8020.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2010.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2011.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221766it [00:27, 8051.91it/s]\n",
      "226715it [00:28, 8089.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2012.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228876it [00:27, 8708.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2013.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255295it [00:31, 8229.06it/s]\n",
      "81119it [00:09, 8080.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2014.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280142it [00:33, 8297.10it/s] \n",
      "303324it [00:35, 8530.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2015.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300925it [00:34, 8704.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2016.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "305754it [00:34, 8899.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2017.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "321636it [00:36, 8869.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2018.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310568it [00:34, 9032.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2019.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2020.tsv.\n",
      "\n",
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2021.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71705it [00:07, 9234.05it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file /home/smildinerm/data/volume_2/data/USPTO/brief_summary/g_brf_sum_text_2022.tsv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357790it [00:41, 8690.21it/s] \n",
      "150625it [00:16, 8816.88it/s]\n",
      "355647it [00:40, 8733.92it/s]\n",
      "326228it [00:37, 8740.94it/s] \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "# input_file = Path(f\"{data_path}/mock\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{storage_path}/output\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{storage_path}/results\")\n",
    "\n",
    "# Parse and compress files:\n",
    "compress_raw_dir_parallel(input_file, year_file, patent_dir, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2633cb",
   "metadata": {},
   "source": [
    "You now have XZ-compressed files containing patents per year. Each file contains a list of JSON-objects, each JSON-object has the following key/values:\n",
    "\n",
    "- `patent`: patent's ID\n",
    "- `file`: path of original text file (not actually used)\n",
    "- `contents`: patent text\n",
    "- `year`: year of publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50576ac",
   "metadata": {},
   "source": [
    "## 2. Calculating embeddings\n",
    "\n",
    "We calculate embeddings and scores with four different models: Countvec, Tf-Idf, Doc2Vec, and BERT ([PatentSBERTa](https://github.com/AI-Growth-Lab/PatentSBERTa)).\n",
    "\n",
    "\n",
    "### 2.1. Preprocessors & parameters\n",
    "Each model has its own preprocessor with various parameters. Most models also have configurable hyperparameters. The values for these parameters have been optimised using the original dataset, resulting in the values used in the `compute_embeddings()`-function below.\n",
    "\n",
    "To recalibrate preprocessor and model parameters, run each model's hyperopt-script. See the [readme](https://github.com/UtrechtUniversity/patent-breakthrough/blob/main/docs/hyperparameter.md) and [hyperopt-notebooks](hyperopt/) for more details.\n",
    "\n",
    "\n",
    "### 2.2. Calculating embeddings\n",
    "Next, we calculate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cef47cf-c7ef-4938-aae1-b96f8d5f12eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{storage_path}/output\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{storage_path}/results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4628b4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smildinerm/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "# from docembedder.preprocessor.oldprep import OldPreprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "from docembedder.models import CountVecEmbedder\n",
    "from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "# def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_cv = {\n",
    "#         \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "#     }\n",
    "#     prep_cv = {\n",
    "#         \"prep-countvec\": OldPreprocessor(list_path=data_path)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "#     print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=False, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated tfidf emdeddings')\n",
    "\n",
    "def compute_embeddings_doc2vec(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_doc2vec = {\n",
    "        \"doc2vec\": D2VEmbedder(epoch=8, min_count=13, vector_size=100)\n",
    "    }\n",
    "    prep_doc2vec = {\n",
    "        \"prep-doc2vec\": Preprocessor(keep_caps=False, keep_start_section=True, remove_non_alpha=False)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_doc2vec, model_doc2vec, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated doc2vec emdeddings')\n",
    "\n",
    "def compute_embeddings_bert(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_bert = {\n",
    "        \"bert\": BERTEmbedder(pretrained_model='AI-Growth-Lab/PatentSBERTa')\n",
    "    }\n",
    "    prep_bert = {\n",
    "         \"prep-bert\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    pretrained_run_models(prep_bert, model_bert, sim_spec, patent_dir, output_fp, cpc_fp)\n",
    "    print('Calculated BERT emdeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f8df3",
   "metadata": {},
   "source": [
    "#### Defining the calculation window\n",
    "\n",
    "Embeddings are calculated within a time window, which shifts over the dataset and then recalculated.\n",
    "This procedure is configured with the `SimulationSpecification()`, which has the following attributes:\n",
    "    \n",
    "- `year_start`: start year of the entire (sub)set of data to calculate embeddings for.\n",
    "- `year_end`: id. end year (the end year itself is not included).\n",
    "- `window_size`: width of the window (in years) to compute embeddings for.\n",
    "- `window_shift`: number of years between subsequent windows.\n",
    "- `debug_max_patents`: restrict the number of patents per year (optional; for testing purposes).\n",
    "    \n",
    "With the `n_jobs`-parameter you can set the number of concurrent jobs to run. A higher number means faster processing, but be aware that each job takes utilises one CPU-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8123f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from docembedder.simspec import SimulationSpecification\n",
    "\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1976,\n",
    "    year_end=2022,\n",
    "    window_size=11,\n",
    "    window_shift=1,\n",
    "    # cpc_samples_per_patent = 10000,\n",
    "    # debug_max_patents = 1000,\n",
    "    n_patents_per_window = 10000,\n",
    ")\n",
    "\n",
    "n_jobs=22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42590c",
   "metadata": {},
   "source": [
    "#### Computing embeddings\n",
    "\n",
    "Now that we've defined the window, we can calculate embeddings, using each of the four models.\n",
    "    \n",
    "Be aware, depending on the amlount of patents and window size, this will take quite some time, \n",
    "and can require a (_very_) large amount of memory. Warnings from the Countvec calculations can be ignored.\n",
    "\n",
    "All output is stored in a HDF5 file, which contains embeddings for all patents in all windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61043e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [1:09:13<1:39:09, 258.69s/it]"
     ]
    }
   ],
   "source": [
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# # Countvec\n",
    "# compute_embeddings_cv(**args)\n",
    "\n",
    "# Tf-Idf\n",
    "compute_embeddings_tfidf(**args)\n",
    "\n",
    "# Doc2Vec\n",
    "compute_embeddings_doc2vec(**args)\n",
    "\n",
    "# BERT\n",
    "compute_embeddings_bert(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b7efe",
   "metadata": {},
   "source": [
    "## 3. Impact and novelty scores\n",
    "\n",
    "### 3.1. Calculating the scores\n",
    "\n",
    "After we've computed and stored the embeddings, we compute novelty and impact scores. The result is a dictionary per model, each containing the novelties and impacts for each patent.\n",
    "\n",
    "\n",
    "_Note on exponents_\n",
    "\n",
    "The exponents (`[1.0, 2.0, 3.0]`) are used in the calculations to reward patents that are more similar to the patent under consideration. The backward and forward similarities for each patent is calculated based on the mean of all cosine similarities with the preceding and following patents in the window, using the formula `(x1**a + x2**a + ...)**(1/a)`, with `a` being the exponent. An `a` larger than 1 increases the weight of similarities closer to 1, i.e. of embeddings that are more similar to the one under consideration. The output includes the result for each exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecd533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 1976-1980\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1977-1981\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1978-1982\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1979-1983\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1980-1984\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1981-1985\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1982-1986\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1983-1987\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1984-1988\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1985-1989\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1986-1990\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1987-1991\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1988-1992\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1989-1993\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1990-1994\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1991-1995\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1992-1996\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1993-1997\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1994-1998\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1995-1999\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1996-2000\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1997-2001\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1998-2002\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1999-2003\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2000-2004\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2001-2005\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2002-2006\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2003-2007\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2004-2008\n",
      "model: prep-tfidf-tfidf\n",
      "window: 2005-2009\n",
      "model: prep-tfidf-tfidf\n"
     ]
    }
   ],
   "source": [
    "from docembedder.analysis import DocAnalysis\n",
    "from docembedder.datamodel import DataModel\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compute_impacts(embedding_fp, output_dir, progr_dir, n_jobs):\n",
    "    exponents = [1.0, 2.0, 3.0]\n",
    "\n",
    "    impact_novel = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    progr_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    with DataModel(embedding_fp, read_only=False) as data:\n",
    "        analysis = DocAnalysis(data)\n",
    "       \n",
    "        for window, model in data.iterate_window_models():\n",
    "            print(f'window: {window}')\n",
    "            print(f'model: {model}')\n",
    "            results = analysis.impact_novelty_results(window, model, exponents, cache=False, n_jobs=n_jobs)\n",
    "\n",
    "            for expon, res in results.items():\n",
    "                if expon == exponents[0]:\n",
    "                    impact_novel[model][\"patent_ids\"].extend(res[\"patent_ids\"])\n",
    "                impact_novel[model][f\"impact-{expon}\"].extend(res[\"impact\"])\n",
    "                impact_novel[model][f\"novelty-{expon}\"].extend(res[\"novelty\"])\n",
    "                \n",
    "                # Save intermediate results as .csv\n",
    "                intermediate_results_fp = Path(progr_dir, f\"results-{model}-{window}-{expon}.csv\")\n",
    "                pd.DataFrame(res).to_csv(intermediate_results_fp, index=False)\n",
    "\n",
    "    # output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for model, data in impact_novel.items():\n",
    "        classifier_name = model.split(\"-\")[-1]\n",
    "        impact_fp = Path(output_dir, f\"impact-{classifier_name}.csv\")\n",
    "        pd.DataFrame(impact_novel[model]).sort_values(\"patent_ids\").to_csv(impact_fp, index=False)\n",
    "\n",
    "\n",
    "progr_fp = Path(f\"{storage_path}/results/intermediate\")\n",
    "\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp, progr_dir = progr_fp, n_jobs=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee098a7",
   "metadata": {},
   "source": [
    "### 3.2. Output\n",
    "\n",
    "After the computations are done, novelty and impact scores are written to CSV-files in the results folder. One file per model, with novelty and impact scores for each exponent. The key column refers back to the patent ID's from the original data.\n",
    "\n",
    "Below is a list of the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36c7747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(path.absolute()) for path in results_fp.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b847ca47-b9d4-4a99-bdc8-283d076f66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_tfidf = pd.read_csv(Path(f\"{results_fp}/impact-tfidf.csv\"))\n",
    "# impact_doc2vec = pd.read_csv(Path(f\"{results_fp}/impact-doc2vec.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5e1858-5d18-4643-a6d9-520149e85a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_ids</th>\n",
       "      <th>impact-1.0</th>\n",
       "      <th>novelty-1.0</th>\n",
       "      <th>impact-2.0</th>\n",
       "      <th>novelty-2.0</th>\n",
       "      <th>impact-3.0</th>\n",
       "      <th>novelty-3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4065812</td>\n",
       "      <td>1.001182</td>\n",
       "      <td>0.477062</td>\n",
       "      <td>1.001191</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>1.001199</td>\n",
       "      <td>0.476735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4065813</td>\n",
       "      <td>1.000580</td>\n",
       "      <td>0.488503</td>\n",
       "      <td>1.000586</td>\n",
       "      <td>0.488458</td>\n",
       "      <td>1.000593</td>\n",
       "      <td>0.488412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4065814</td>\n",
       "      <td>1.000898</td>\n",
       "      <td>0.491731</td>\n",
       "      <td>1.000902</td>\n",
       "      <td>0.491686</td>\n",
       "      <td>1.000907</td>\n",
       "      <td>0.491641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4065815</td>\n",
       "      <td>1.000458</td>\n",
       "      <td>0.491266</td>\n",
       "      <td>1.000462</td>\n",
       "      <td>0.491225</td>\n",
       "      <td>1.000467</td>\n",
       "      <td>0.491182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4065816</td>\n",
       "      <td>1.000488</td>\n",
       "      <td>0.481946</td>\n",
       "      <td>1.000486</td>\n",
       "      <td>0.481885</td>\n",
       "      <td>1.000483</td>\n",
       "      <td>0.481823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_ids  impact-1.0  novelty-1.0  impact-2.0  novelty-2.0  impact-3.0  \\\n",
       "0     4065812    1.001182     0.477062    1.001191     0.476900    1.001199   \n",
       "1     4065813    1.000580     0.488503    1.000586     0.488458    1.000593   \n",
       "2     4065814    1.000898     0.491731    1.000902     0.491686    1.000907   \n",
       "3     4065815    1.000458     0.491266    1.000462     0.491225    1.000467   \n",
       "4     4065816    1.000488     0.481946    1.000486     0.481885    1.000483   \n",
       "\n",
       "   novelty-3.0  \n",
       "0     0.476735  \n",
       "1     0.488412  \n",
       "2     0.491641  \n",
       "3     0.491182  \n",
       "4     0.481823  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c06cd2-6864-475e-b222-ec0be62beba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_ids</th>\n",
       "      <th>impact-1.0</th>\n",
       "      <th>novelty-1.0</th>\n",
       "      <th>impact-2.0</th>\n",
       "      <th>novelty-2.0</th>\n",
       "      <th>impact-3.0</th>\n",
       "      <th>novelty-3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.292881e+06</td>\n",
       "      <td>9.999473e-01</td>\n",
       "      <td>4.810577e-01</td>\n",
       "      <td>9.999486e-01</td>\n",
       "      <td>4.809243e-01</td>\n",
       "      <td>9.999500e-01</td>\n",
       "      <td>4.807864e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.863551e+06</td>\n",
       "      <td>1.038546e-03</td>\n",
       "      <td>4.979888e-03</td>\n",
       "      <td>1.054899e-03</td>\n",
       "      <td>5.022688e-03</td>\n",
       "      <td>1.072521e-03</td>\n",
       "      <td>5.067377e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.065812e+06</td>\n",
       "      <td>9.929912e-01</td>\n",
       "      <td>4.507686e-01</td>\n",
       "      <td>9.926143e-01</td>\n",
       "      <td>4.505437e-01</td>\n",
       "      <td>9.922072e-01</td>\n",
       "      <td>4.503165e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.679864e+06</td>\n",
       "      <td>9.993010e-01</td>\n",
       "      <td>4.777420e-01</td>\n",
       "      <td>9.992952e-01</td>\n",
       "      <td>4.775796e-01</td>\n",
       "      <td>9.992892e-01</td>\n",
       "      <td>4.774110e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.292353e+06</td>\n",
       "      <td>9.999189e-01</td>\n",
       "      <td>4.811134e-01</td>\n",
       "      <td>9.999192e-01</td>\n",
       "      <td>4.809825e-01</td>\n",
       "      <td>9.999195e-01</td>\n",
       "      <td>4.808478e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.905024e+06</td>\n",
       "      <td>1.000541e+00</td>\n",
       "      <td>4.844514e-01</td>\n",
       "      <td>1.000548e+00</td>\n",
       "      <td>4.843498e-01</td>\n",
       "      <td>1.000556e+00</td>\n",
       "      <td>4.842445e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.052440e+07</td>\n",
       "      <td>1.006098e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.006192e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.006302e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         patent_ids    impact-1.0   novelty-1.0    impact-2.0   novelty-2.0  \\\n",
       "count  6.409309e+06  6.409309e+06  6.409309e+06  6.409309e+06  6.409309e+06   \n",
       "mean   7.292881e+06  9.999473e-01  4.810577e-01  9.999486e-01  4.809243e-01   \n",
       "std    1.863551e+06  1.038546e-03  4.979888e-03  1.054899e-03  5.022688e-03   \n",
       "min    4.065812e+06  9.929912e-01  4.507686e-01  9.926143e-01  4.505437e-01   \n",
       "25%    5.679864e+06  9.993010e-01  4.777420e-01  9.992952e-01  4.775796e-01   \n",
       "50%    7.292353e+06  9.999189e-01  4.811134e-01  9.999192e-01  4.809825e-01   \n",
       "75%    8.905024e+06  1.000541e+00  4.844514e-01  1.000548e+00  4.843498e-01   \n",
       "max    1.052440e+07  1.006098e+00  5.000000e-01  1.006192e+00  5.000000e-01   \n",
       "\n",
       "         impact-3.0   novelty-3.0  \n",
       "count  6.409309e+06  6.409309e+06  \n",
       "mean   9.999500e-01  4.807864e-01  \n",
       "std    1.072521e-03  5.067377e-03  \n",
       "min    9.922072e-01  4.503165e-01  \n",
       "25%    9.992892e-01  4.774110e-01  \n",
       "50%    9.999195e-01  4.808478e-01  \n",
       "75%    1.000556e+00  4.842445e-01  \n",
       "max    1.006302e+00  5.000000e-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_tfidf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4d9aea-ca12-40b1-a1a3-84eb8af43089",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'impact_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.subplot()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_doc2vec[\"impact-1.0\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# plt.scatter(x=impact_doc2vec[\"impact-1.0\"],y=impact_doc2vec[\"impact-3.0\"])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x\u001b[38;5;241m=\u001b[39m\u001b[43mimpact_tfidf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpact-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m],y\u001b[38;5;241m=\u001b[39mimpact_tfidf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnovelty-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'impact_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot()\n",
    "# plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_doc2vec[\"impact-1.0\"])\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(x=impact_doc2vec[\"impact-1.0\"],y=impact_doc2vec[\"impact-3.0\"])\n",
    "# plt.show()\n",
    "\n",
    "plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_tfidf[\"novelty-1.0\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b481dc5-98c6-4ce0-9e18-5041e2bd0b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
