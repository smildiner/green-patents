{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2839077a",
   "metadata": {},
   "source": [
    "# Patent Breakthrough walkthrough\n",
    "\n",
    "This notebook illustrates the complete analysis process of breakthrough patents, from preparing input files\n",
    "to calculating impact and novelty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317e63",
   "metadata": {},
   "source": [
    "## 1. Preparing input files\n",
    "\n",
    "There are three input files: a file with patent texts, a patent/year-index, and list of patent/CPC-codes.\n",
    "\n",
    "### Patents\n",
    "In its raw format, the input file contains the text of one patent file per line.\n",
    "Each line starts with a path pointing to that patent's original text \n",
    "file (`/Volumes/External/txt/0000000-0100000/US1009.txt`), followed by the patent text. Example file: `./data/raw_input.txt`. \n",
    "\n",
    "\n",
    "### Patent/Year-index\n",
    "Contains the year of publication of each patent. Example file: `./data/year.csv`. \n",
    "\n",
    "\n",
    "### CPC-file\n",
    "The CPC-file (Cooperative Patent Classification) contains the patent classification code for each patent. These codes are used to calculate benchmark similarities. Example file: `./data/GPCPCs.txt`\n",
    "\n",
    "Note: the included data files only contain a small subset of the original data, for example purposes.\n",
    "\n",
    "#### Other files\n",
    "The three other files in the data folder - `greek.txt`  `stopwords.txt`, and `symbols.txt` - are required by the `OldPreprocessor`-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1048ccd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/USPTO\"\n",
    "input_file = Path(f\"{data_path}/brief_text\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "output_folder = Path(\"./output\")\n",
    "output_fp = Path(\"./output\", \"patents.h5\")\n",
    "results_fp = Path(\"./results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69345ab9",
   "metadata": {},
   "source": [
    "### 1.1. Compressing\n",
    "\n",
    "The compressor function transforms the patents to a more manageable format, sorts and saves them by year of publication, and compresses the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f1a427-8a5e-4ee1-8f75-45bd8d074cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c0ffaf2-33b6-4ab3-af74-2502e7fd4752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1976.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 29233.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1977.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 26973.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1980.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 26750.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1981.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 24865.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1979.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 27788.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse file data/USPTO/mock/g_brf_sum_text_1978.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 28484.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import lzma\n",
    "import csv\n",
    "from typing import List, Union, Dict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_xz(compressed_fp: Union[Path, str]) -> List[Dict]:\n",
    "    \"\"\"Read an .xz file containing patents\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    compressed_fp:\n",
    "        File to read the patents from.\n",
    "\n",
    "    Results\n",
    "    -------\n",
    "    patents: List[Dict]\n",
    "        Patents in the file.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"rb\") as handle:\n",
    "        patents = json.loads(handle.read().decode(encoding=\"utf-8\"))\n",
    "    return patents\n",
    "\n",
    "def write_xz(compressed_fp: Union[Path, str], patents: List[Dict]) -> None:\n",
    "    \"\"\"Write a set of patents to a compressed file\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    fp:\n",
    "        File to write to.\n",
    "    patents:\n",
    "        Patents to store.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"wb\", preset=9) as handle:\n",
    "        handle.write(str.encode(json.dumps(patents), encoding=\"utf-8\"))\n",
    "    \n",
    "#     # Get the total number of patents for progress tracking\n",
    "#     total_patents = len(patents)\n",
    "\n",
    "#     # Open the compressed file for writing\n",
    "#     with lzma.open(compressed_fp, mode=\"wb\", preset=9) as handle:\n",
    "#         # Use tqdm to create a progress bar\n",
    "#         with tqdm(total=total_patents, desc=\"Writing patents to compressed file\", unit=\"patent\") as pbar:\n",
    "#             # Iterate through patents and write to the compressed file\n",
    "#             for patent in patents:\n",
    "#                 handle.write(str.encode(json.dumps(patent), encoding=\"utf-8\"))\n",
    "#                 pbar.update(1)  # Update the progress bar\n",
    "\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_patent_file(patent_input_fp: str, year_lookup: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a raw patent file into a structured list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    patent_input_fp : str\n",
    "        Path to the .tsv input file to process with columns 'patent_id' and 'summary_text', one patent per row.\n",
    "\n",
    "    year_lookup : str\n",
    "        Path to the .csv file to lookup the year for each patent ID with columns 'pat' (patient_id) and 'year'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict[str, str]]\n",
    "        A sorted list of dictionaries, where each item is for one patent and\n",
    "        the year of publication:\n",
    "        {\n",
    "            'patent': patent_id,\n",
    "            'file': file name,\n",
    "            'contents':  summary_text,\n",
    "            'year': year\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting to parse file {patent_input_fp}\")\n",
    "    \n",
    "    # Increase the field size limit\n",
    "    csv.field_size_limit(10**8)\n",
    "    \n",
    "    # Initialize an empty list to store the structured patent data\n",
    "    patents = []\n",
    "\n",
    "    # Read the year lookup data into a dictionary for quick access\n",
    "    year_dict = {}\n",
    "    with open(year_lookup, 'r', newline='') as year_file:\n",
    "        year_reader = csv.DictReader(year_file, delimiter='\\t')\n",
    "        for row in year_reader:\n",
    "            year_dict[row['pat']] = row['year']  # Convert 'year' to integer\n",
    "\n",
    "    # Read the raw patent file and parse the data\n",
    "    with open(patent_input_fp, 'r', newline='') as patent_file:\n",
    "        patent_reader = csv.DictReader(patent_file, delimiter='\\t')\n",
    "        for row in tqdm(patent_reader):\n",
    "            patent_id = row['patent_id']\n",
    "            \n",
    "            # Skip entries where \"patent_id\" is not an integer\n",
    "            if not patent_id.isdigit():\n",
    "                continue\n",
    "\n",
    "            summary_text = row['summary_text']\n",
    "            year = year_dict.get(patent_id, None)\n",
    "\n",
    "            # Create a dictionary for the current patent\n",
    "            patent_data = {\n",
    "                'patent': int(patent_id),  # Convert 'patent_id' to integer\n",
    "                'file': str(patent_input_fp),\n",
    "                'contents': summary_text,\n",
    "                'year': int(year)\n",
    "            }\n",
    "\n",
    "            # Append the patent data to the list\n",
    "            patents.append(patent_data)\n",
    "    \n",
    "    patents = sorted(patents, key=lambda x: x[\"patent\"])\n",
    "    \n",
    "    return patents\n",
    "\n",
    "\n",
    "def compress_raw_dir(patent_input_dir: Union[Path, str], year_fp: Union[Path, str], output_dir: Union[Path, str]) -> None:\n",
    "    \"\"\"Compress all raw files in a directory.\n",
    "\n",
    "    For efficiency, it stores which files have already been processed in\n",
    "    a file called 'processed_files.txt' in the output directory.\n",
    "    If somehow there is corruption, or re-runs are required, simply\n",
    "    delete this file.\n",
    "\n",
    "    This function is not thread-safe.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    patent_input_dir:\n",
    "        Directory containing all raw files with patents.\n",
    "    year_fp:\n",
    "        CSV file with publication year for each patent.\n",
    "    output_dir:\n",
    "        Directory to write the compressed files to.\n",
    "    \"\"\"\n",
    "    patent_input_dir = Path(patent_input_dir)\n",
    "    \n",
    "    for file in patent_input_dir.glob(\"g_brf_sum_text_*.tsv\"):\n",
    "        \n",
    "        # Extract year from the filename\n",
    "        year = str(file).split('_')[-1].split('.')[0]\n",
    "        \n",
    "        # Get next file path to process and compress\n",
    "        patent_input_fp = file\n",
    "        compressed_fp = output_dir / Path(str(year) + \".xz\")\n",
    "        \n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(compressed_fp):\n",
    "            print(f\"Output file {compressed_fp} already exists. Skipping to the next input file.\")\n",
    "            continue\n",
    "\n",
    "        # Parse patents for a year in the right format\n",
    "        patents = parse_patent_file(patent_input_fp, year_fp)\n",
    "\n",
    "        # Write the patents to a files, numbered by year\n",
    "        write_xz(compressed_fp, patents)\n",
    "    \n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "data_path = \"./data/USPTO\"\n",
    "# input_file = Path(f\"{data_path}/brief_text\")\n",
    "input_file = Path(f\"{data_path}/mock\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "\n",
    "# Parse and compress files:\n",
    "compress_raw_dir(input_file, year_file, patent_dir)\n",
    "\n",
    "# Now 'parsed_patents' contains the structured list of patents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2633cb",
   "metadata": {},
   "source": [
    "You now have XZ-compressed files containing patents per year. Each file contains a list of JSON-objects, each JSON-object has the following key/values:\n",
    "\n",
    "- `patent`: patent's ID\n",
    "- `file`: path of original text file (not actually used)\n",
    "- `contents`: patent text\n",
    "- `year`: year of publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50576ac",
   "metadata": {},
   "source": [
    "## 2. Calculating embeddings\n",
    "\n",
    "We calculate embeddings and scores with four different models: Countvec, Tf-Idf, Doc2Vec, and BERT ([PatentSBERTa](https://github.com/AI-Growth-Lab/PatentSBERTa)).\n",
    "\n",
    "\n",
    "### 2.1. Preprocessors & parameters\n",
    "Each model has its own preprocessor with various parameters. Most models also have configurable hyperparameters. The values for these parameters have been optimised using the original dataset, resulting in the values used in the `compute_embeddings()`-function below.\n",
    "\n",
    "To recalibrate preprocessor and model parameters, run each model's hyperopt-script. See the [readme](https://github.com/UtrechtUniversity/patent-breakthrough/blob/main/docs/hyperparameter.md) and [hyperopt-notebooks](hyperopt/) for more details.\n",
    "\n",
    "\n",
    "### 2.2. Calculating embeddings\n",
    "Next, we calculate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cef47cf-c7ef-4938-aae1-b96f8d5f12eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/USPTO\"\n",
    "input_file = Path(f\"{data_path}/brief_text\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "output_folder = Path(\"./output\")\n",
    "output_fp = Path(\"./output\", \"patents.h5\")\n",
    "results_fp = Path(\"./results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4628b4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "from docembedder.preprocessor.oldprep import OldPreprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "# from docembedder.models import CountVecEmbedder\n",
    "# from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "# def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_cv = {\n",
    "#         \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "#     }\n",
    "#     prep_cv = {\n",
    "#         \"prep-countvec\": OldPreprocessor(list_path=data_path)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "#     print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=False, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated tfidf emdeddings')\n",
    "\n",
    "    \n",
    "# def compute_embeddings_doc2vec(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_doc2vec = {\n",
    "#         \"doc2vec\": D2VEmbedder(epoch=8, min_count=13, vector_size=100)\n",
    "#     }\n",
    "#     prep_doc2vec = {\n",
    "#         \"prep-doc2vec\": Preprocessor(keep_caps=False, keep_start_section=True, remove_non_alpha=False)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     run_models(prep_doc2vec, model_doc2vec, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "#     print('Calculated doc2vec emdeddings')\n",
    "\n",
    "# def compute_embeddings_bert(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_bert = {\n",
    "#         \"bert\": BERTEmbedder(pretrained_model='AI-Growth-Lab/PatentSBERTa')\n",
    "#     }\n",
    "#     prep_bert = {\n",
    "#          \"prep-bert\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     pretrained_run_models(prep_bert, model_bert, sim_spec, patent_dir, output_fp, cpc_fp)\n",
    "#     print('Calculated BERT emdeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f8df3",
   "metadata": {},
   "source": [
    "#### Defining the calculation window\n",
    "\n",
    "Embeddings are calculated within a time window, which shifts over the dataset and then recalculated.\n",
    "This procedure is configured with the `SimulationSpecification()`, which has the following attributes:\n",
    "    \n",
    "- `year_start`: start year of the entire (sub)set of data to calculate embeddings for.\n",
    "- `year_end`: id. end year (the end year itself is not included).\n",
    "- `window_size`: width of the window (in years) to compute embeddings for.\n",
    "- `window_shift`: number of years between subsequent windows.\n",
    "- `debug_max_patents`: restrict the number of patents per year (optional; for testing purposes).\n",
    "    \n",
    "With the `n_jobs`-parameter you can set the number of concurrent jobs to run. A higher number means faster processing, but be aware that each job takes utilises one CPU-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8123f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from docembedder.simspec import SimulationSpecification\n",
    "\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1976,\n",
    "    year_end=1980,\n",
    "    window_size=2,\n",
    "    window_shift=1,\n",
    "    cpc_samples_per_patent = 20,\n",
    "    debug_max_patents = 100,\n",
    "    # n_patents_per_window = 100,\n",
    ")\n",
    "\n",
    "n_jobs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42590c",
   "metadata": {},
   "source": [
    "#### Computing embeddings\n",
    "\n",
    "Now that we've defined the window, we can calculate embeddings, using each of the four models.\n",
    "    \n",
    "Be aware, depending on the amlount of patents and window size, this will take quite some time, \n",
    "and can require a (_very_) large amount of memory. Warnings from the Countvec calculations can be ignored.\n",
    "\n",
    "All output is stored in a HDF5 file, which contains embeddings for all patents in all windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61043e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████                                 | 1/4 [02:15<06:47, 135.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     85\u001b[0m args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatent_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: patent_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_fp\u001b[39m\u001b[38;5;124m'\u001b[39m: output_fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpc_fp\u001b[39m\u001b[38;5;124m'\u001b[39m: cpc_fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msim_spec\u001b[39m\u001b[38;5;124m'\u001b[39m: sim_spec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: n_jobs}\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# # Countvec\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# compute_embeddings_cv(**args)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Tf-Idf\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m compute_embeddings_tfidf(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mcompute_embeddings_tfidf\u001b[0;34m(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs)\u001b[0m\n\u001b[1;32m     60\u001b[0m prep_tfidf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep-tfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m: Preprocessor(keep_caps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, keep_start_section\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_non_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     62\u001b[0m }\n\u001b[1;32m     64\u001b[0m check_files(sim_spec)\n\u001b[0;32m---> 65\u001b[0m run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/utils.py:403\u001b[0m, in \u001b[0;36mrun_models\u001b[0;34m(preprocessors, models, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs, progress_bar)\u001b[0m\n\u001b[1;32m    401\u001b[0m jobs \u001b[38;5;241m=\u001b[39m create_jobs(sim_spec, output_fp, models, preprocessors, cpc_fp, patent_dir)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m     run_jobs_single(jobs, output_fp, progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     run_jobs_multi(jobs, output_fp, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar)\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/utils.py:359\u001b[0m, in \u001b[0;36mrun_jobs_single\u001b[0;34m(jobs, output_fp, progress_bar)\u001b[0m\n\u001b[1;32m    357\u001b[0m all_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m tqdm(jobs, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress_bar):\n\u001b[0;32m--> 359\u001b[0m     temp_data_fp \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    360\u001b[0m     all_files\u001b[38;5;241m.\u001b[39mappend(temp_data_fp)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Merge the files with the main output file.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/utils.py:202\u001b[0m, in \u001b[0;36mJob.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Compute the CPC correlations\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_cpc:\n\u001b[0;32m--> 202\u001b[0m     cpc_cor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cpc(patent_id)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     cpc_cor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/utils.py:145\u001b[0m, in \u001b[0;36mJob.compute_cpc\u001b[0;34m(self, test_id)\u001b[0m\n\u001b[1;32m    143\u001b[0m test_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_id)\n\u001b[1;32m    144\u001b[0m pat_class \u001b[38;5;241m=\u001b[39m PatentClassification(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpc_fp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 145\u001b[0m cpc_cor \u001b[38;5;241m=\u001b[39m pat_class\u001b[38;5;241m.\u001b[39msample_cpc_correlations(\n\u001b[1;32m    146\u001b[0m     test_id, samples_per_patent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_spec\u001b[38;5;241m.\u001b[39mcpc_samples_per_patent,\n\u001b[1;32m    147\u001b[0m     seed\u001b[38;5;241m=\u001b[39mavg_year)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cpc_cor\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/classification.py:168\u001b[0m, in \u001b[0;36mPatentClassification.sample_cpc_correlations\u001b[0;34m(self, patent_ids, samples_per_patent, seed)\u001b[0m\n\u001b[1;32m    166\u001b[0m     i_patents \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(i_patent_list)\n\u001b[1;32m    167\u001b[0m     j_patents \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(j_patent_list)\n\u001b[0;32m--> 168\u001b[0m correlations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_similarity(patent_ids[i_pat], patent_ids[j_pat])\n\u001b[1;32m    169\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i_pat, j_pat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(i_patents, j_patents)]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m: i_patents,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mj_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m: j_patents,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrelations\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(correlations),\n\u001b[1;32m    174\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/classification.py:168\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    166\u001b[0m     i_patents \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(i_patent_list)\n\u001b[1;32m    167\u001b[0m     j_patents \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(j_patent_list)\n\u001b[0;32m--> 168\u001b[0m correlations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_similarity(patent_ids[i_pat], patent_ids[j_pat])\n\u001b[1;32m    169\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i_pat, j_pat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(i_patents, j_patents)]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m: i_patents,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mj_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m: j_patents,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrelations\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(correlations),\n\u001b[1;32m    174\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/classification.py:50\u001b[0m, in \u001b[0;36mPatentClassification.get_similarity\u001b[0;34m(self, i_patent_id, j_patent_id)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_patent_id \u001b[38;5;241m==\u001b[39m j_patent_id:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m i_patent_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pat_classifications(i_patent_id)\n\u001b[1;32m     51\u001b[0m j_patent_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pat_classifications(j_patent_id)\n\u001b[1;32m     52\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(i_patent_class), \u001b[38;5;28mlen\u001b[39m(j_patent_class)))\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/classification.py:62\u001b[0m, in \u001b[0;36mPatentClassification._get_pat_classifications\u001b[0;34m(self, patent_id)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_pat_classifications\u001b[39m(\u001b[38;5;28mself\u001b[39m, patent_id) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_patent_ids()\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Utrecht University/PhD/Projects/DS/patents/patent-breakthrough/docembedder/classification.py:118\u001b[0m, in \u001b[0;36mPatentClassification.set_patent_ids\u001b[0;34m(self, patent_ids)\u001b[0m\n\u001b[1;32m    116\u001b[0m     query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mjoin(pat_df\u001b[38;5;241m.\u001b[39mlazy(), on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpat\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    117\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mcollect()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df_filtered[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpat\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_filtered[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/polars/series/series.py:3863\u001b[0m, in \u001b[0;36mSeries.to_list\u001b[0;34m(self, use_pyarrow)\u001b[0m\n\u001b[1;32m   3860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_pyarrow:\n\u001b[1;32m   3861\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_arrow()\u001b[38;5;241m.\u001b[39mto_pylist()\n\u001b[0;32m-> 3863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s\u001b[38;5;241m.\u001b[39mto_list()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/USPTO\"\n",
    "input_file = Path(f\"{data_path}/brief_text\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "output_folder = Path(\"./output\")\n",
    "output_fp = Path(\"./output\", \"patents.h5\")\n",
    "results_fp = Path(\"./results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "from docembedder.preprocessor.oldprep import OldPreprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "from docembedder.models import CountVecEmbedder\n",
    "from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "# def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_cv = {\n",
    "#         \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "#     }\n",
    "#     prep_cv = {\n",
    "#         \"prep-countvec\": OldPreprocessor(list_path=data_path)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "#     print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=False, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "from docembedder.simspec import SimulationSpecification\n",
    "\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1976,\n",
    "    year_end=1982,\n",
    "    window_size=3,\n",
    "    window_shift=1,\n",
    "    cpc_samples_per_patent = 10,\n",
    "    debug_max_patents = 100,\n",
    "    # n_patents_per_window = 100,\n",
    ")\n",
    "\n",
    "n_jobs=1    \n",
    "\n",
    "\n",
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# # Countvec\n",
    "# compute_embeddings_cv(**args)\n",
    "\n",
    "# Tf-Idf\n",
    "compute_embeddings_tfidf(**args)\n",
    "\n",
    "# # Doc2Vec\n",
    "# compute_embeddings_doc2vec(**args)\n",
    "\n",
    "# # BERT\n",
    "# compute_embeddings_bert(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b7efe",
   "metadata": {},
   "source": [
    "## 3. Impact and novelty scores\n",
    "\n",
    "### 3.1. Calculating the scores\n",
    "\n",
    "After we've computed and stored the embeddings, we compute novelty and impact scores. The result is a dictionary per model, each containing the novelties and impacts for each patent.\n",
    "\n",
    "\n",
    "_Note on exponents_\n",
    "\n",
    "The exponents (`[1.0, 2.0, 3.0]`) are used in the calculations to reward patents that are more similar to the patent under consideration. The backward and forward similarities for each patent is calculated based on the mean of all cosine similarities with the preceding and following patents in the window, using the formula `(x1**a + x2**a + ...)**(1/a)`, with `a` being the exponent. An `a` larger than 1 increases the weight of similarities closer to 1, i.e. of embeddings that are more similar to the one under consideration. The output includes the result for each exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cecd533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 1976-1978\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1977-1979\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1978-1980\n",
      "model: prep-tfidf-tfidf\n",
      "window: 1979-1981\n",
      "model: prep-tfidf-tfidf\n"
     ]
    }
   ],
   "source": [
    "from docembedder.analysis import DocAnalysis\n",
    "from docembedder.datamodel import DataModel\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compute_impacts(embedding_fp, output_dir):\n",
    "    exponents = [1.0, 2.0, 3.0]\n",
    "\n",
    "    impact_novel = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    with DataModel(embedding_fp, read_only=False) as data:\n",
    "        analysis = DocAnalysis(data)\n",
    "       \n",
    "        for window, model in data.iterate_window_models():\n",
    "            print(f'window: {window}')\n",
    "            print(f'model: {model}')\n",
    "            results = analysis.impact_novelty_results(window, model, exponents, cache=False, n_jobs=1)\n",
    "\n",
    "            for expon, res in results.items():\n",
    "                if expon == exponents[0]:\n",
    "                    impact_novel[model][\"patent_ids\"].extend(res[\"patent_ids\"])\n",
    "                impact_novel[model][f\"impact-{expon}\"].extend(res[\"impact\"])\n",
    "                impact_novel[model][f\"novelty-{expon}\"].extend(res[\"novelty\"])\n",
    "\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for model, data in impact_novel.items():\n",
    "        classifier_name = model.split(\"-\")[-1]\n",
    "        impact_fp = Path(output_dir, f\"impact-{classifier_name}.csv\")\n",
    "        pd.DataFrame(impact_novel[model]).sort_values(\"patent_ids\").to_csv(impact_fp, index=False)\n",
    "\n",
    "\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee098a7",
   "metadata": {},
   "source": [
    "### 3.2. Output\n",
    "\n",
    "After the computations are done, novelty and impact scores are written to CSV-files in the results folder. One file per model, with novelty and impact scores for each exponent. The key column refers back to the patent ID's from the original data.\n",
    "\n",
    "Below is a list of the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36c7747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/a6159737/Documents/Utrecht University/PhD/Projects/DS/patents/results/impact-tfidf.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(path.absolute()) for path in results_fp.iterdir()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
