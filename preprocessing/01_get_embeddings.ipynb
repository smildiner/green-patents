{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2839077a",
   "metadata": {},
   "source": [
    "# Patent Breakthrough walkthrough\n",
    "\n",
    "This notebook illustrates the complete analysis process of breakthrough patents, from preparing input files\n",
    "to calculating impact and novelty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317e63",
   "metadata": {},
   "source": [
    "## 1. Preparing input files\n",
    "\n",
    "There are three input files: a file with patent texts, a patent/year-index, and list of patent/CPC-codes.\n",
    "\n",
    "### Patents\n",
    "In its raw format, the input file contains the text of one patent file per line.\n",
    "Each line starts with a path pointing to that patent's original text \n",
    "file (`/Volumes/External/txt/0000000-0100000/US1009.txt`), followed by the patent text. Example file: `./data/raw_input.txt`. \n",
    "\n",
    "\n",
    "### Patent/Year-index\n",
    "Contains the year of publication of each patent. Example file: `./data/year.csv`. \n",
    "\n",
    "\n",
    "### CPC-file\n",
    "The CPC-file (Cooperative Patent Classification) contains the patent classification code for each patent. These codes are used to calculate benchmark similarities. Example file: `./data/GPCPCs.txt`\n",
    "\n",
    "Note: the included data files only contain a small subset of the original data, for example purposes.\n",
    "\n",
    "#### Other files\n",
    "The three other files in the data folder - `greek.txt`  `stopwords.txt`, and `symbols.txt` - are required by the `OldPreprocessor`-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1048ccd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{home_path}/output\")\n",
    "output_fp = Path(f\"{home_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{home_path}/results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69345ab9",
   "metadata": {},
   "source": [
    "### 1.1. Compressing\n",
    "\n",
    "The compressor function transforms the patents to a more manageable format, sorts and saves them by year of publication, and compresses the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0ffaf2-33b6-4ab3-af74-2502e7fd4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from typing import List, Union, Dict\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import multiprocessing\n",
    "\n",
    "def read_xz(compressed_fp: Union[Path, str]) -> List[Dict]:\n",
    "    \"\"\"Read an .xz file containing patents\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    compressed_fp:\n",
    "        File to read the patents from.\n",
    "\n",
    "    Results\n",
    "    -------\n",
    "    patents: List[Dict]\n",
    "        Patents in the file.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"rb\") as handle:\n",
    "        patents = json.loads(handle.read().decode(encoding=\"utf-8\"))\n",
    "    return patents\n",
    "\n",
    "def write_xz(compressed_fp: Union[Path, str], patents: List[Dict]) -> None:\n",
    "    \"\"\"Write a set of patents to a compressed file\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    fp:\n",
    "        File to write to.\n",
    "    patents:\n",
    "        Patents to store.\n",
    "    \"\"\"\n",
    "    with lzma.open(compressed_fp, mode=\"wb\", preset=9) as handle:\n",
    "        handle.write(str.encode(json.dumps(patents), encoding=\"utf-8\"))\n",
    "\n",
    "def parse_patent_file(patent_input_fp: str, year_lookup: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a raw patent file into a structured list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    patent_input_fp : str\n",
    "        Path to the .tsv input file to process with columns 'patent_id' and 'summary_text', one patent per row.\n",
    "\n",
    "    year_lookup : str\n",
    "        Path to the .csv file to lookup the year for each patent ID with columns 'pat' (patient_id) and 'year'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict[str, str]]\n",
    "        A sorted list of dictionaries, where each item is for one patent and\n",
    "        the year of publication:\n",
    "        {\n",
    "            'patent': patent_id,\n",
    "            'file': file name,\n",
    "            'contents':  summary_text,\n",
    "            'year': year\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting to parse file {patent_input_fp}.\\n\")\n",
    "    \n",
    "    # Increase the field size limit\n",
    "    csv.field_size_limit(10**8)\n",
    "    \n",
    "    # Initialize an empty list to store the structured patent data\n",
    "    patents = []\n",
    "\n",
    "    # Read the year lookup data into a dictionary for quick access\n",
    "    year_dict = {}\n",
    "    with open(year_lookup, 'r', newline='') as year_file:\n",
    "        year_reader = csv.DictReader(year_file, delimiter='\\t')\n",
    "        for row in year_reader:\n",
    "            year_dict[row['pat']] = row['year']  # Convert 'year' to integer\n",
    "\n",
    "    # Extract year from the filename\n",
    "    year = str(patent_input_fp).split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Read the raw patent file and parse the data\n",
    "    with open(patent_input_fp, 'r', newline='') as patent_file:\n",
    "        patent_reader = csv.DictReader(patent_file, delimiter='\\t')\n",
    "        for row in tqdm(patent_reader):\n",
    "            patent_id = row['patent_id']\n",
    "            \n",
    "            # Skip entries where \"patent_id\" is not an integer\n",
    "            if not patent_id.isdigit():\n",
    "                continue\n",
    "\n",
    "            # Skip entries where \"patent_id\" is not in year_file:\n",
    "            if patent_id not in year_dict:\n",
    "                continue\n",
    "\n",
    "            summary_text = row['summary_text']\n",
    "            # year = year_dict.get(patent_id, None)\n",
    "\n",
    "            # Create a dictionary for the current patent\n",
    "            patent_data = {\n",
    "                'patent': int(patent_id),  # Convert 'patent_id' to integer\n",
    "                'file': str(patent_input_fp),\n",
    "                'contents': summary_text,\n",
    "                'year': int(year)\n",
    "            }\n",
    "\n",
    "            # Append the patent data to the list\n",
    "            patents.append(patent_data)\n",
    "    \n",
    "    patents = sorted(patents, key=lambda x: x[\"patent\"])\n",
    "    \n",
    "    return patents\n",
    "\n",
    "def compress_raw_dir(patent_input_dir: Union[Path, str], year_fp: Union[Path, str], output_dir: Union[Path, str]) -> None:\n",
    "    \"\"\"Compress all raw files in a directory.\n",
    "\n",
    "    For efficiency, it stores which files have already been processed in\n",
    "    a file called 'processed_files.txt' in the output directory.\n",
    "    If somehow there is corruption, or re-runs are required, simply\n",
    "    delete this file.\n",
    "\n",
    "    This function is not thread-safe.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    patent_input_dir:\n",
    "        Directory containing all raw files with patents.\n",
    "    year_fp:\n",
    "        CSV file with publication year for each patent.\n",
    "    output_dir:\n",
    "        Directory to write the compressed files to.\n",
    "    \"\"\"\n",
    "    patent_input_dir = Path(patent_input_dir)\n",
    "    \n",
    "    for file in patent_input_dir.glob(\"g_brf_sum_text_*.tsv\"):\n",
    "        \n",
    "        # Extract year from the filename\n",
    "        year = str(file).split('_')[-1].split('.')[0]\n",
    "        \n",
    "        # Get next file path to process and compress\n",
    "        patent_input_fp = file\n",
    "        compressed_fp = output_dir / Path(str(year) + \".xz\")\n",
    "        \n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(compressed_fp):\n",
    "            print(f\"Output file {compressed_fp} already exists. Skipping to the next input file.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Parse patents for a year in the right format\n",
    "        patents = parse_patent_file(patent_input_fp, year_fp)\n",
    "\n",
    "        # Write the patents to a files, numbered by year\n",
    "        write_xz(compressed_fp, patents)\n",
    "\n",
    "\n",
    "def compress_and_write(patent_input_fp: Path, year_fp: Path, output_dir: Path) -> None:\n",
    "    \"\"\"Compresses patents from a single file and writes the compressed data to a new file.\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    patent_input_fp (Path):\n",
    "        Path to the input patent file.\n",
    "    year_fp (Path):\n",
    "        Path to the CSV file with publication year for each patent.\n",
    "    output_dir (Path):\n",
    "        Directory to write the compressed files to.\n",
    "    \"\"\"\n",
    "    year = str(patent_input_fp).split('_')[-1].split('.')[0]\n",
    "    compressed_fp = output_dir / Path(str(year) + \".xz\")\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(compressed_fp):\n",
    "        print(f\"Output file {compressed_fp} already exists. Skipping to the next input file.\\n\")\n",
    "        return\n",
    "\n",
    "    # Parse patents for a year in the right format\n",
    "    patents = parse_patent_file(patent_input_fp, year_fp)\n",
    "\n",
    "    # Write the patents to a file, compressed\n",
    "    write_xz(compressed_fp, patents)\n",
    "\n",
    "\n",
    "def compress_raw_dir_parallel(patent_input_dir: Union[Path, str], year_fp: Union[Path, str], output_dir: Union[Path, str], num_cores: int) -> None:\n",
    "    \"\"\"Compresses patents from multiple files in parallel using multiprocessing.\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    patent_input_dir (Union[Path, str]):\n",
    "        Directory containing all raw files with patents.\n",
    "    year_fp (Union[Path, str]):\n",
    "        CSV file with publication year for each patent.\n",
    "    output_dir (Union[Path, str]):\n",
    "        Directory to write the compressed files to.\n",
    "    num_cores (int):\n",
    "        Number of CPU cores to use for parallel processing.\n",
    "    \"\"\"\n",
    "    patent_input_dir = Path(patent_input_dir)\n",
    "    year_fp = Path(year_fp)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    patent_files = list(patent_input_dir.glob(\"g_brf_sum_text_*.tsv\"))\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "        pool.starmap(compress_and_write, [(file, year_fp, output_dir) for file in patent_files])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823caaa-6a5e-49f1-abac-18d3cd78a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file /home/smildinerm/data/volume_2/patents/1977.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1976.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1978.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1979.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1981.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1983.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1984.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1986.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1988.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/1989.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1982.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1987.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1992.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1991.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1985.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/1990.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/1995.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1994.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1993.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/1996.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/1980.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1999.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2000.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1997.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2002.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/1998.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2005.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2003.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2007.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2008.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2009.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2006.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2011.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2010.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2012.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2013.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2001.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2014.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2016.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2017.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2019.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2004.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2020.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2021.xz already exists. Skipping to the next input file.\n",
      "Output file /home/smildinerm/data/volume_2/patents/2022.xz already exists. Skipping to the next input file.\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2015.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output file /home/smildinerm/data/volume_2/patents/2018.xz already exists. Skipping to the next input file.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "# input_file = Path(f\"{data_path}/mock\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{storage_path}/output\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{storage_path}/results\")\n",
    "\n",
    "# Parse and compress files:\n",
    "compress_raw_dir_parallel(input_file, year_file, patent_dir, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2633cb",
   "metadata": {},
   "source": [
    "You now have XZ-compressed files containing patents per year. Each file contains a list of JSON-objects, each JSON-object has the following key/values:\n",
    "\n",
    "- `patent`: patent's ID\n",
    "- `file`: path of original text file (not actually used)\n",
    "- `contents`: patent text\n",
    "- `year`: year of publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50576ac",
   "metadata": {},
   "source": [
    "## 2. Calculating embeddings\n",
    "\n",
    "We calculate embeddings and scores with four different models: Countvec, Tf-Idf, Doc2Vec, and BERT ([PatentSBERTa](https://github.com/AI-Growth-Lab/PatentSBERTa)).\n",
    "\n",
    "\n",
    "### 2.1. Preprocessors & parameters\n",
    "Each model has its own preprocessor with various parameters. Most models also have configurable hyperparameters. The values for these parameters have been optimised using the original dataset, resulting in the values used in the `compute_embeddings()`-function below.\n",
    "\n",
    "To recalibrate preprocessor and model parameters, run each model's hyperopt-script. See the [readme](https://github.com/UtrechtUniversity/patent-breakthrough/blob/main/docs/hyperparameter.md) and [hyperopt-notebooks](hyperopt/) for more details.\n",
    "\n",
    "\n",
    "### 2.2. Calculating embeddings\n",
    "Next, we calculate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cef47cf-c7ef-4938-aae1-b96f8d5f12eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home_path = \"/home/smildinerm\"\n",
    "storage_path = \"/home/smildinerm/data/volume_2\"\n",
    "data_path = \"/home/smildinerm/data/volume_2/data/USPTO\"\n",
    "\n",
    "input_file = Path(f\"{data_path}/brief_summary\")\n",
    "year_file = Path(f\"{data_path}/input_files/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/input_files/GPCPCs.txt\")\n",
    "patent_dir = Path(f\"{storage_path}/patents\")\n",
    "output_folder = Path(f\"{storage_path}/output\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents.h5\")\n",
    "results_fp = Path(f\"{storage_path}/results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4628b4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smildinerm/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import lzma\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "# from docembedder.preprocessor.oldprep import OldPreprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "from docembedder.models import CountVecEmbedder\n",
    "from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "# def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "#     model_cv = {\n",
    "#         \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "#     }\n",
    "#     prep_cv = {\n",
    "#         \"prep-countvec\": OldPreprocessor(list_path=data_path)\n",
    "#     }\n",
    "\n",
    "#     check_files(sim_spec)\n",
    "#     run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "#     print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=False, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated tfidf emdeddings')\n",
    "\n",
    "def compute_embeddings_doc2vec(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_doc2vec = {\n",
    "        \"doc2vec\": D2VEmbedder(epoch=8, min_count=13, vector_size=100)\n",
    "    }\n",
    "    prep_doc2vec = {\n",
    "        \"prep-doc2vec\": Preprocessor(keep_caps=False, keep_start_section=True, remove_non_alpha=False)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_doc2vec, model_doc2vec, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated doc2vec emdeddings')\n",
    "\n",
    "def compute_embeddings_bert(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_bert = {\n",
    "        \"bert\": BERTEmbedder(pretrained_model='AI-Growth-Lab/PatentSBERTa')\n",
    "    }\n",
    "    prep_bert = {\n",
    "         \"prep-bert\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    pretrained_run_models(prep_bert, model_bert, sim_spec, patent_dir, output_fp, cpc_fp)\n",
    "    print('Calculated BERT emdeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f8df3",
   "metadata": {},
   "source": [
    "#### Defining the calculation window\n",
    "\n",
    "Embeddings are calculated within a time window, which shifts over the dataset and then recalculated.\n",
    "This procedure is configured with the `SimulationSpecification()`, which has the following attributes:\n",
    "    \n",
    "- `year_start`: start year of the entire (sub)set of data to calculate embeddings for.\n",
    "- `year_end`: id. end year (the end year itself is not included).\n",
    "- `window_size`: width of the window (in years) to compute embeddings for.\n",
    "- `window_shift`: number of years between subsequent windows.\n",
    "- `debug_max_patents`: restrict the number of patents per year (optional; for testing purposes).\n",
    "    \n",
    "With the `n_jobs`-parameter you can set the number of concurrent jobs to run. A higher number means faster processing, but be aware that each job takes utilises one CPU-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8123f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from docembedder.simspec import SimulationSpecification\n",
    "\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1996,\n",
    "    # year_start=2005,\n",
    "    year_end=2023,\n",
    "    window_size=11,\n",
    "    window_shift=1,\n",
    "    # cpc_samples_per_patent = 10000,\n",
    "    # debug_max_patents = 1000,\n",
    "    # n_patents_per_window = 10000,\n",
    ")\n",
    "\n",
    "n_jobs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42590c",
   "metadata": {},
   "source": [
    "#### Computing embeddings\n",
    "\n",
    "Now that we've defined the window, we can calculate embeddings, using each of the four models.\n",
    "    \n",
    "Be aware, depending on the amlount of patents and window size, this will take quite some time, \n",
    "and can require a (_very_) large amount of memory. Warnings from the Countvec calculations can be ignored.\n",
    "\n",
    "All output is stored in a HDF5 file, which contains embeddings for all patents in all windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61043e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [5:05:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't synchronously write data (file write failed: time = Thu Feb 29 02:39:02 2024\n, filename = '/home/smildinerm/data/volume_2/output/temp_2005-2015.h5', file descriptor = 5, errno = 28, error message = 'No space left on device', buf = 0x7f9e9b4e5fc8, total write size = 4337667848, bytes this sub-write = 4337667848, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/docembedder/utils.py\", line 299, in _pool_worker\n    return job.run()\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/docembedder/utils.py\", line 207, in run\n    self.store_results(temp_fp, window_name, patent_id, patent_year, cpc_cor, all_embeddings)\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/docembedder/utils.py\", line 172, in store_results\n    data.store_embeddings(window_name, model_name, embeddings)\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/docembedder/datamodel.py\", line 154, in store_embeddings\n    dataset_group.create_dataset(\"data\", data=embeddings.data)\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/h5py/_hl/group.py\", line 183, in create_dataset\n    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)\n  File \"/home/smildinerm/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py\", line 166, in make_new_dset\n    dset_id.write(h5s.ALL, h5s.ALL, data)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5d.pyx\", line 282, in h5py.h5d.DatasetID.write\n  File \"h5py/_proxy.pyx\", line 115, in h5py._proxy.dset_rw\nOSError: [Errno 28] Can't synchronously write data (file write failed: time = Thu Feb 29 02:39:02 2024\n, filename = '/home/smildinerm/data/volume_2/output/temp_2005-2015.h5', file descriptor = 5, errno = 28, error message = 'No space left on device', buf = 0x7f9e9b4e5fc8, total write size = 4337667848, bytes this sub-write = 4337667848, bytes actually written = 18446744073709551615, offset = 0)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatent_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: patent_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_fp\u001b[39m\u001b[38;5;124m'\u001b[39m: output_fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpc_fp\u001b[39m\u001b[38;5;124m'\u001b[39m: cpc_fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msim_spec\u001b[39m\u001b[38;5;124m'\u001b[39m: sim_spec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: n_jobs}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Tf-Idf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcompute_embeddings_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mcompute_embeddings_tfidf\u001b[0;34m(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs)\u001b[0m\n\u001b[1;32m     45\u001b[0m prep_tfidf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep-tfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m: Preprocessor(keep_caps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, keep_start_section\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_non_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     47\u001b[0m }\n\u001b[1;32m     49\u001b[0m check_files(sim_spec)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mrun_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpc_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculated tfidf emdeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/utils.py:405\u001b[0m, in \u001b[0;36mrun_models\u001b[0;34m(preprocessors, models, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs, progress_bar)\u001b[0m\n\u001b[1;32m    403\u001b[0m     run_jobs_single(jobs, output_fp, progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[43mrun_jobs_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/utils.py:326\u001b[0m, in \u001b[0;36mrun_jobs_multi\u001b[0;34m(jobs, output_fp, n_jobs, progress_bar)\u001b[0m\n\u001b[1;32m    324\u001b[0m all_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m temp_data_fp \u001b[38;5;129;01min\u001b[39;00m tqdm(pool\u001b[38;5;241m.\u001b[39mimap_unordered(_pool_worker, jobs),\n\u001b[1;32m    327\u001b[0m                              total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(jobs),\n\u001b[1;32m    328\u001b[0m                              disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress_bar):\n\u001b[1;32m    329\u001b[0m         all_files\u001b[38;5;241m.\u001b[39mappend(temp_data_fp)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Merge the files with the main output file.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:868\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 868\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't synchronously write data (file write failed: time = Thu Feb 29 02:39:02 2024\n, filename = '/home/smildinerm/data/volume_2/output/temp_2005-2015.h5', file descriptor = 5, errno = 28, error message = 'No space left on device', buf = 0x7f9e9b4e5fc8, total write size = 4337667848, bytes this sub-write = 4337667848, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_tfidf.h5\")\n",
    "# output_fp = Path(f\"{storage_path}/output\", \"patents_2005-2022_11yrw_tfidf.h5\")\n",
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# Tf-Idf\n",
    "compute_embeddings_tfidf(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c83a4-8a64-43c9-8a3a-736e42644ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_doc2vec.h5\")\n",
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# Doc2Vec\n",
    "compute_embeddings_doc2vec(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb999a9-f18c-4810-a591-183992658a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_bert.h5\")\n",
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# BERT\n",
    "compute_embeddings_bert(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b7efe",
   "metadata": {},
   "source": [
    "## 3. Impact and novelty scores\n",
    "\n",
    "### 3.1. Calculating the scores\n",
    "\n",
    "After we've computed and stored the embeddings, we compute novelty and impact scores. The result is a dictionary per model, each containing the novelties and impacts for each patent.\n",
    "\n",
    "\n",
    "_Note on exponents_\n",
    "\n",
    "The exponents (`[1.0, 2.0, 3.0]`) are used in the calculations to reward patents that are more similar to the patent under consideration. The backward and forward similarities for each patent is calculated based on the mean of all cosine similarities with the preceding and following patents in the window, using the formula `(x1**a + x2**a + ...)**(1/a)`, with `a` being the exponent. An `a` larger than 1 increases the weight of similarities closer to 1, i.e. of embeddings that are more similar to the one under consideration. The output includes the result for each exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cecd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.analysis import DocAnalysis\n",
    "from docembedder.datamodel import DataModel\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def compute_impacts(embedding_fp, output_dir, progr_dir, n_jobs):\n",
    "    exponents = [1.0, 5.0, 10.0]\n",
    "\n",
    "    impact_novel = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    progr_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    with DataModel(embedding_fp, read_only=False) as data:\n",
    "        analysis = DocAnalysis(data)\n",
    "        \n",
    "        # Use tqdm for progress tracking\n",
    "        window_models = list(data.iterate_window_models())\n",
    "        for window, model in tqdm(window_models, desc=\"Processing windows\"):\n",
    "            results = analysis.impact_novelty_results(window, model, exponents, cache=False, n_jobs=n_jobs)\n",
    "\n",
    "            for expon, res in results.items():\n",
    "                if expon == exponents[0]:\n",
    "                    impact_novel[model][\"patent_ids\"].extend(res[\"patent_ids\"])\n",
    "                impact_novel[model][f\"impact-{expon}\"].extend(res[\"impact\"])\n",
    "                impact_novel[model][f\"novelty-{expon}\"].extend(res[\"novelty\"])\n",
    "                \n",
    "                # Save intermediate results as .csv\n",
    "                intermediate_results_fp = Path(progr_dir, f\"results-{model}-{window}-{expon}.csv\")\n",
    "                pd.DataFrame(res).to_csv(intermediate_results_fp, index=False)\n",
    "\n",
    "    for model, data in impact_novel.items():\n",
    "        classifier_name = model.split(\"-\")[-1]\n",
    "        impact_fp = Path(output_dir, f\"impact-{classifier_name}.csv\")\n",
    "        pd.DataFrame(impact_novel[model]).sort_values(\"patent_ids\").to_csv(impact_fp, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a0332-3249-43e8-bf90-6b212fd51abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing windows:  12%|█▏        | 2/17 [23:50:23<179:13:17, 43013.19s/it]"
     ]
    }
   ],
   "source": [
    "progr_fp = Path(f\"{storage_path}/results/intermediate\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_tfidf.h5\")\n",
    "\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp, progr_dir = progr_fp, n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696cfcb4-5c2e-4b7b-91ed-ba6e37eab8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the HDF5 file:\n",
      "['cpc', 'embeddings', 'impact_novelty', 'models', 'preprocessors', 'windows']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_tfidf.h5\")\n",
    "\n",
    "with h5py.File(output_fp, 'r') as file:\n",
    "    print(\"Keys in the HDF5 file:\")\n",
    "    print(list(file.keys()))\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(output_fp, 'r') as file:\n",
    "    # Access the dataset(s) within the file\n",
    "    cpc = file['cpc']\n",
    "    embeddings = file['embeddings']\n",
    "    impact_novelty = file['impact_novelty']\n",
    "    models = file['models']\n",
    "    preprocessors = file['preprocessors']\n",
    "    windows = file['windows']\n",
    "\n",
    "    # # Read the data from the dataset(s)\n",
    "    # cpc = cpc[:]\n",
    "    # embeddings = embeddings[:]\n",
    "    # impact_novelty = impact_novelty[:]\n",
    "    # models = models[:]\n",
    "    # preprocessors = preprocessors[:]\n",
    "    # windows = windows[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db1d791-f359-4f48-a995-f9d25e4a65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the 'windows' group:\n",
      "['1996-2006', '1997-2007', '1998-2008', '1999-2009', '2000-2010', '2001-2011', '2002-2012', '2003-2013', '2004-2014', '2005-2015', '2006-2016', '2007-2017', '2008-2018', '2009-2019', '2010-2020', '2011-2021', '2012-2022']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_fp, 'r') as file:\n",
    "    windows_group = file['windows']\n",
    "    print(\"Keys in the 'windows' group:\")\n",
    "    print(list(windows_group.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f1e9de-6d48-4c4d-b212-ea945d6d96a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/smildinerm/data/volume_2/output/patents_1996-2022_11yrw_tfidf.h5\n",
      "Group: cpc\n",
      "  Group: 1996-2006\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 1997-2007\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 1998-2008\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 1999-2009\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2000-2010\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2001-2011\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2002-2012\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2003-2013\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2004-2014\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2005-2015\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2006-2016\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2007-2017\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2008-2018\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2009-2019\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2010-2020\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2011-2021\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "  Group: 2012-2022\n",
      "    Dataset: correlations\n",
      "    Dataset: i_patents\n",
      "    Dataset: j_patents\n",
      "Group: embeddings\n",
      "  Group: prep-tfidf-tfidf\n",
      "    Group: 1996-2006\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 1997-2007\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 1998-2008\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 1999-2009\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2000-2010\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2001-2011\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2002-2012\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2003-2013\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2004-2014\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2005-2015\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2006-2016\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2007-2017\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2008-2018\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2009-2019\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2010-2020\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2011-2021\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "    Group: 2012-2022\n",
      "      Dataset: data\n",
      "      Dataset: indices\n",
      "      Dataset: indptr\n",
      "Group: impact_novelty\n",
      "Group: models\n",
      "  Group: tfidf\n",
      "Group: preprocessors\n",
      "  Group: prep-tfidf\n",
      "Group: windows\n",
      "  Group: 1996-2006\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 1997-2007\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 1998-2008\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 1999-2009\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2000-2010\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2001-2011\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2002-2012\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2003-2013\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2004-2014\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2005-2015\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2006-2016\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2007-2017\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2008-2018\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2009-2019\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2010-2020\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2011-2021\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n",
      "  Group: 2012-2022\n",
      "    Dataset: patent_id\n",
      "    Dataset: year\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_hdf5_structure(group, indent=0):\n",
    "    \"\"\"Recursively print the structure of an HDF5 group.\"\"\"\n",
    "    for key in group.keys():\n",
    "        print(\"  \" * indent + f\"{'Group' if isinstance(group[key], h5py.Group) else 'Dataset'}: {key}\")\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            print_hdf5_structure(group[key], indent + 1)\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(output_fp, 'r') as file:\n",
    "    # Print the overall structure\n",
    "    print(f\"File: {file.filename}\")\n",
    "    print_hdf5_structure(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18600e47-1609-49c2-8cfa-b3b6e7f23b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the 'windows' group:\n",
      "['prep-tfidf-tfidf']\n",
      "['1976-1980', '1977-1981', '1978-1982', '1979-1983', '1980-1984', '1981-1985', '1982-1986', '1983-1987', '1984-1988', '1985-1989', '1986-1990', '1987-1991', '1988-1992', '1989-1993', '1990-1994', '1991-1995', '1992-1996', '1993-1997', '1994-1998', '1995-1999', '1996-2000', '1997-2001', '1998-2002', '1999-2003', '2000-2004', '2001-2005', '2002-2006', '2003-2007', '2004-2008', '2005-2009', '2006-2010', '2007-2011', '2008-2012', '2009-2013', '2010-2014', '2011-2015', '2012-2016', '2013-2017', '2014-2018', '2015-2019', '2016-2020', '2017-2021']\n",
      "['data', 'indices', 'indptr']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71460467"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fp = Path(f\"{storage_path}/output\", \"patents_tfidf_1976-2022_5yrw.h5\")\n",
    "\n",
    "with h5py.File(output_fp, 'r') as file:\n",
    "    group = file['embeddings']\n",
    "    print(\"Keys in the 'windows' group:\")\n",
    "    print(list(group.keys()))\n",
    "    subgroup = group['prep-tfidf-tfidf']\n",
    "    print(list(subgroup.keys()))\n",
    "    subsubgroup = subgroup['1976-1980']\n",
    "    print(list(subsubgroup.keys()))\n",
    "    data = subsubgroup['data'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6ece6c-592f-48c2-bb76-37bec78c6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis: <docembedder.analysis.DocAnalysis object at 0x7fa77087b4f0>\n",
      "window: 1996-2006\n",
      "model: prep-doc2vec-doc2vec\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 9970 is out of bounds for axis 0 with size 9969",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/analysis.py:230\u001b[0m, in \u001b[0;36mDocAnalysis.impact_novelty_results\u001b[0;34m(self, window_name, model_name, exponents, cache, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m expon \u001b[38;5;129;01min\u001b[39;00m exponents:\n\u001b[0;32m--> 230\u001b[0m         results[expon] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_impact_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/datamodel.py:347\u001b[0m, in \u001b[0;36mDataModel.load_impact_novelty\u001b[0;34m(self, window_name, model_name, exponent)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load impacts for a window/year.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mArguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    list of impacts for that window/model.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m dataset_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/impact_novelty/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexponent\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    348\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpact\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpact\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnovelty\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnovelty\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexponent\u001b[39m\u001b[38;5;124m\"\u001b[39m: exponent,\n\u001b[1;32m    354\u001b[0m }\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:189\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unable to synchronously open object (component not found)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m progr_fp \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/results/intermediate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m output_fp \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/output\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatents_1996-2022_11yrw_doc2vec.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompute_impacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_fp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogr_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprogr_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mcompute_impacts\u001b[0;34m(embedding_fp, output_dir, progr_dir, n_jobs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpact_novelty_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexponents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expon, res \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expon \u001b[38;5;241m==\u001b[39m exponents[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/analysis.py:232\u001b[0m, in \u001b[0;36mDocAnalysis.impact_novelty_results\u001b[0;34m(self, window_name, model_name, exponents, cache, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         results[expon] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload_impact_novelty(window_name, model_name, expon)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_impact_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexponents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m expon \u001b[38;5;129;01min\u001b[39;00m exponents:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/analysis.py:182\u001b[0m, in \u001b[0;36mDocAnalysis.compute_impact_novelty\u001b[0;34m(self, window_name, model_name, window, exponents, n_jobs, max_mat_size)\u001b[0m\n\u001b[1;32m    177\u001b[0m back_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((patent_years \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m focal_year \u001b[38;5;241m-\u001b[39m window[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    178\u001b[0m                     \u001b[38;5;241m&\u001b[39m (patent_years \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m focal_year\u001b[38;5;241m-\u001b[39mwindow[\u001b[38;5;241m1\u001b[39m]))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    179\u001b[0m forw_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((patent_years \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m focal_year \u001b[38;5;241m+\u001b[39m window[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;241m&\u001b[39m (patent_years \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m focal_year\u001b[38;5;241m+\u001b[39mwindow[\u001b[38;5;241m1\u001b[39m]))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 182\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_impact_novelty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mback_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforw_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mat_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expon \u001b[38;5;129;01min\u001b[39;00m exponents:\n\u001b[1;32m    186\u001b[0m     results[expon][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfocal_year\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m focal_year\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/docembedder/analysis.py:104\u001b[0m, in \u001b[0;36mcompute_impact_novelty\u001b[0;34m(embeddings, back_idx, focal_idx, forw_idx, n_jobs, max_mat_size, exponents, progress_bar)\u001b[0m\n\u001b[1;32m    101\u001b[0m     exponents \u001b[38;5;241m=\u001b[39m [exponents]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Normalize the embeddings.\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m embeddings_backward \u001b[38;5;241m=\u001b[39m normalize(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mback_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    105\u001b[0m embeddings_forward \u001b[38;5;241m=\u001b[39m normalize(embeddings[forw_idx])\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Figure out over how many jobs the focal embeddings should be split.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9970 is out of bounds for axis 0 with size 9969"
     ]
    }
   ],
   "source": [
    "progr_fp = Path(f\"{storage_path}/results/intermediate\")\n",
    "output_fp = Path(f\"{storage_path}/output\", \"patents_1996-2022_11yrw_doc2vec.h5\")\n",
    "\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp, progr_dir = progr_fp, n_jobs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee098a7",
   "metadata": {},
   "source": [
    "### 3.2. Output\n",
    "\n",
    "After the computations are done, novelty and impact scores are written to CSV-files in the results folder. One file per model, with novelty and impact scores for each exponent. The key column refers back to the patent ID's from the original data.\n",
    "\n",
    "Below is a list of the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36c7747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(path.absolute()) for path in results_fp.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b847ca47-b9d4-4a99-bdc8-283d076f66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_tfidf = pd.read_csv(Path(f\"{results_fp}/impact-tfidf.csv\"))\n",
    "# impact_doc2vec = pd.read_csv(Path(f\"{results_fp}/impact-doc2vec.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5e1858-5d18-4643-a6d9-520149e85a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_ids</th>\n",
       "      <th>impact-1.0</th>\n",
       "      <th>novelty-1.0</th>\n",
       "      <th>impact-2.0</th>\n",
       "      <th>novelty-2.0</th>\n",
       "      <th>impact-3.0</th>\n",
       "      <th>novelty-3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4065812</td>\n",
       "      <td>1.001182</td>\n",
       "      <td>0.477062</td>\n",
       "      <td>1.001191</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>1.001199</td>\n",
       "      <td>0.476735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4065813</td>\n",
       "      <td>1.000580</td>\n",
       "      <td>0.488503</td>\n",
       "      <td>1.000586</td>\n",
       "      <td>0.488458</td>\n",
       "      <td>1.000593</td>\n",
       "      <td>0.488412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4065814</td>\n",
       "      <td>1.000898</td>\n",
       "      <td>0.491731</td>\n",
       "      <td>1.000902</td>\n",
       "      <td>0.491686</td>\n",
       "      <td>1.000907</td>\n",
       "      <td>0.491641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4065815</td>\n",
       "      <td>1.000458</td>\n",
       "      <td>0.491266</td>\n",
       "      <td>1.000462</td>\n",
       "      <td>0.491225</td>\n",
       "      <td>1.000467</td>\n",
       "      <td>0.491182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4065816</td>\n",
       "      <td>1.000488</td>\n",
       "      <td>0.481946</td>\n",
       "      <td>1.000486</td>\n",
       "      <td>0.481885</td>\n",
       "      <td>1.000483</td>\n",
       "      <td>0.481823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_ids  impact-1.0  novelty-1.0  impact-2.0  novelty-2.0  impact-3.0  \\\n",
       "0     4065812    1.001182     0.477062    1.001191     0.476900    1.001199   \n",
       "1     4065813    1.000580     0.488503    1.000586     0.488458    1.000593   \n",
       "2     4065814    1.000898     0.491731    1.000902     0.491686    1.000907   \n",
       "3     4065815    1.000458     0.491266    1.000462     0.491225    1.000467   \n",
       "4     4065816    1.000488     0.481946    1.000486     0.481885    1.000483   \n",
       "\n",
       "   novelty-3.0  \n",
       "0     0.476735  \n",
       "1     0.488412  \n",
       "2     0.491641  \n",
       "3     0.491182  \n",
       "4     0.481823  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c06cd2-6864-475e-b222-ec0be62beba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_ids</th>\n",
       "      <th>impact-1.0</th>\n",
       "      <th>novelty-1.0</th>\n",
       "      <th>impact-2.0</th>\n",
       "      <th>novelty-2.0</th>\n",
       "      <th>impact-3.0</th>\n",
       "      <th>novelty-3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "      <td>6.409309e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.292881e+06</td>\n",
       "      <td>9.999473e-01</td>\n",
       "      <td>4.810577e-01</td>\n",
       "      <td>9.999486e-01</td>\n",
       "      <td>4.809243e-01</td>\n",
       "      <td>9.999500e-01</td>\n",
       "      <td>4.807864e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.863551e+06</td>\n",
       "      <td>1.038546e-03</td>\n",
       "      <td>4.979888e-03</td>\n",
       "      <td>1.054899e-03</td>\n",
       "      <td>5.022688e-03</td>\n",
       "      <td>1.072521e-03</td>\n",
       "      <td>5.067377e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.065812e+06</td>\n",
       "      <td>9.929912e-01</td>\n",
       "      <td>4.507686e-01</td>\n",
       "      <td>9.926143e-01</td>\n",
       "      <td>4.505437e-01</td>\n",
       "      <td>9.922072e-01</td>\n",
       "      <td>4.503165e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.679864e+06</td>\n",
       "      <td>9.993010e-01</td>\n",
       "      <td>4.777420e-01</td>\n",
       "      <td>9.992952e-01</td>\n",
       "      <td>4.775796e-01</td>\n",
       "      <td>9.992892e-01</td>\n",
       "      <td>4.774110e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.292353e+06</td>\n",
       "      <td>9.999189e-01</td>\n",
       "      <td>4.811134e-01</td>\n",
       "      <td>9.999192e-01</td>\n",
       "      <td>4.809825e-01</td>\n",
       "      <td>9.999195e-01</td>\n",
       "      <td>4.808478e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.905024e+06</td>\n",
       "      <td>1.000541e+00</td>\n",
       "      <td>4.844514e-01</td>\n",
       "      <td>1.000548e+00</td>\n",
       "      <td>4.843498e-01</td>\n",
       "      <td>1.000556e+00</td>\n",
       "      <td>4.842445e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.052440e+07</td>\n",
       "      <td>1.006098e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.006192e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.006302e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         patent_ids    impact-1.0   novelty-1.0    impact-2.0   novelty-2.0  \\\n",
       "count  6.409309e+06  6.409309e+06  6.409309e+06  6.409309e+06  6.409309e+06   \n",
       "mean   7.292881e+06  9.999473e-01  4.810577e-01  9.999486e-01  4.809243e-01   \n",
       "std    1.863551e+06  1.038546e-03  4.979888e-03  1.054899e-03  5.022688e-03   \n",
       "min    4.065812e+06  9.929912e-01  4.507686e-01  9.926143e-01  4.505437e-01   \n",
       "25%    5.679864e+06  9.993010e-01  4.777420e-01  9.992952e-01  4.775796e-01   \n",
       "50%    7.292353e+06  9.999189e-01  4.811134e-01  9.999192e-01  4.809825e-01   \n",
       "75%    8.905024e+06  1.000541e+00  4.844514e-01  1.000548e+00  4.843498e-01   \n",
       "max    1.052440e+07  1.006098e+00  5.000000e-01  1.006192e+00  5.000000e-01   \n",
       "\n",
       "         impact-3.0   novelty-3.0  \n",
       "count  6.409309e+06  6.409309e+06  \n",
       "mean   9.999500e-01  4.807864e-01  \n",
       "std    1.072521e-03  5.067377e-03  \n",
       "min    9.922072e-01  4.503165e-01  \n",
       "25%    9.992892e-01  4.774110e-01  \n",
       "50%    9.999195e-01  4.808478e-01  \n",
       "75%    1.000556e+00  4.842445e-01  \n",
       "max    1.006302e+00  5.000000e-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_tfidf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4d9aea-ca12-40b1-a1a3-84eb8af43089",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'impact_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.subplot()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_doc2vec[\"impact-1.0\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# plt.scatter(x=impact_doc2vec[\"impact-1.0\"],y=impact_doc2vec[\"impact-3.0\"])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x\u001b[38;5;241m=\u001b[39m\u001b[43mimpact_tfidf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpact-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m],y\u001b[38;5;241m=\u001b[39mimpact_tfidf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnovelty-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'impact_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot()\n",
    "# plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_doc2vec[\"impact-1.0\"])\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(x=impact_doc2vec[\"impact-1.0\"],y=impact_doc2vec[\"impact-3.0\"])\n",
    "# plt.show()\n",
    "\n",
    "plt.scatter(x=impact_tfidf[\"impact-1.0\"],y=impact_tfidf[\"novelty-1.0\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b481dc5-98c6-4ce0-9e18-5041e2bd0b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
